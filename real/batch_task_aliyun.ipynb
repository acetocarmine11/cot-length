{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e7410e",
   "metadata": {},
   "source": [
    "# API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46773082",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashscope_api_key = \"sk-b69887d9a9244554892e7589af2f187c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e03974",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'integer_below_min_value', 'param': 'n', 'message': '<400> InternalError.Algo.InvalidParameter: Range of n should be [1, 4]', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-c92fdf56-c06d-9d25-98bb-460cdd401842', 'request_id': 'c92fdf56-c06d-9d25-98bb-460cdd401842'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mmodel_dump_json())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mget_response\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m():\n\u001b[1;32m      5\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      6\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mdashscope_api_key,  \u001b[38;5;66;03m# 如果您没有配置环境变量，请用阿里云百炼API Key将本行替换为：api_key=\"sk-xxx\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://dashscope.aliyuncs.com/compatible-mode/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 填写DashScope SDK的base_url\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen2.5-7b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlease reason step by step, and put your final answer within \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mboxed\u001b[39;49m\u001b[38;5;124;43m{{\u001b[39;49m\u001b[38;5;124;43m}}.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1+1=?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mmodel_dump_json())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'integer_below_min_value', 'param': 'n', 'message': '<400> InternalError.Algo.InvalidParameter: Range of n should be [1, 4]', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-c92fdf56-c06d-9d25-98bb-460cdd401842', 'request_id': 'c92fdf56-c06d-9d25-98bb-460cdd401842'}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def get_response():\n",
    "    client = OpenAI(\n",
    "        api_key=dashscope_api_key,  # 如果您没有配置环境变量，请用阿里云百炼API Key将本行替换为：api_key=\"sk-xxx\"\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope SDK的base_url\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen2.5-7b-instruct\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        messages=[{'role': 'system', 'content': 'Please reason step by step, and put your final answer within \\\\boxed{{}}.'},\n",
    "                  {'role': 'user', 'content': '1+1=?'}],\n",
    "        )\n",
    "    print(completion.model_dump_json())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f52c0d",
   "metadata": {},
   "source": [
    "# prepare input data\n",
    "\n",
    "{\"custom_id\":\"2\",\"method\":\"POST\",\"url\":\"/v1/chat/completions\",\"body\":{\"model\":\"qwen2.5-7b-instruct\",\"messages\":[{\"role\":\"system\",\"content\":\"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"},{\"role\":\"user\",\"content\":problem}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d97edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成 test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"math500.jsonl\"\n",
    "output_path = \"test.jsonl\"\n",
    "system_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, line in enumerate(fin):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except Exception as e:\n",
    "            print(f\"第{idx+1}行解析失败: {e}\")\n",
    "            continue\n",
    "        problem = obj.get(\"problem\", \"\")\n",
    "        out_obj = {\n",
    "            \"custom_id\": str(idx + 1),\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"qwen2.5-7b-instruct\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": problem}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        fout.write(json.dumps(out_obj, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"已生成 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bb3c1",
   "metadata": {},
   "source": [
    "# Upload file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440d6f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"file-batch-89cbe2460f1d4037bbb5e441\",\"bytes\":234290,\"created_at\":1753614009,\"filename\":\"test.jsonl\",\"object\":\"file\",\"purpose\":\"batch\",\"status\":\"processed\",\"status_details\":null}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，可用阿里云百炼API Key将下行替换为：api_key=\"sk-xxx\"。但不建议在生产环境中直接将API Key硬编码到代码中，以减少API Key泄露风险。\n",
    "    api_key=dashscope_api_key,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 阿里云百炼服务的base_url\n",
    ")\n",
    "\n",
    "# test.jsonl 是一个本地示例文件，purpose必须是batch\n",
    "file_object = client.files.create(file=Path(\"test.jsonl\"), purpose=\"batch\")\n",
    "\n",
    "print(file_object.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f559b38",
   "metadata": {},
   "source": [
    "# Create batch task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a018e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_90e6f1e9-a5e4-451d-ba1e-25c6aaa768d0', completion_window='24h', created_at=1753614176, endpoint='/v1/chat/completions', input_file_id='file-batch-89cbe2460f1d4037bbb5e441', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=None, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'ds_name': 'math500-qwen7b', 'ds_description': '任务描述'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.create(\n",
    "    input_file_id=\"file-batch-89cbe2460f1d4037bbb5e441\",  # 上传文件返回的id或OSS文件URL或OSS文件资源标识符\n",
    "    endpoint=\"/v1/chat/completions\",  # Embedding文本向量模型填写/v1/embeddings,测试模型batch-test-model填写/v1/chat/ds-test,其他模型填写/v1/chat/completions\n",
    "    completion_window=\"24h\",\n",
    "    metadata={'ds_name':\"math500-qwen7b\",'ds_description':'任务描述'} # metadata数据，非必填字段，用于创建任务名称、描述\n",
    ")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea1845",
   "metadata": {},
   "source": [
    "# Manage batch task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c327e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_90e6f1e9-a5e4-451d-ba1e-25c6aaa768d0', completion_window='24h', created_at=1753614176, endpoint='/v1/chat/completions', input_file_id='file-batch-89cbe2460f1d4037bbb5e441', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='model_not_found', line=1, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=2, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=3, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=4, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=5, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=6, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=7, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=8, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=9, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=10, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=11, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=12, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=13, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=14, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=15, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=16, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=17, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=18, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=19, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model'), BatchError(code='model_not_found', line=20, message=\"The provided model 'qwen2.5-7b-instruct' is not supported by the Batch API.\", param='body.model')], object='list'), expired_at=None, expires_at=None, failed_at=1753614176, finalizing_at=None, in_progress_at=None, metadata={'ds_name': 'math500-qwen7b', 'ds_description': '任务描述'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "batch = client.batches.retrieve(\"batch_90e6f1e9-a5e4-451d-ba1e-25c6aaa768d0\")  # 将batch_id替换为Batch任务的id\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09d22b",
   "metadata": {},
   "source": [
    "# test winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee84c3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac6254ecfca4a2fb115a922f104fda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36928469a7274d5bae87a006c0399994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "winogrande_xs/train-00000-of-00001.parqu(…):   0%|          | 0.00/11.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c7f08090094e229318af64cf68a885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "winogrande_xs/test-00000-of-00001.parque(…):   0%|          | 0.00/118k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bba98c210434b2a9c5be6bd50b7ee48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "winogrande_xs/validation-00000-of-00001.(…):   0%|          | 0.00/85.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688af75e61e64975abf74fbabadf5cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f2f872a4de40be916d5bf5f02b9c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c49b16682714a6ab406dd48920db471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'option1', 'option2', 'answer'],\n",
      "        num_rows: 160\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'option1', 'option2', 'answer'],\n",
      "        num_rows: 1767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'option1', 'option2', 'answer'],\n",
      "        num_rows: 1267\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载Winogrande数据集\n",
    "winogrande_dataset = load_dataset(\"winogrande\", \"winogrande_xs\")\n",
    "print(winogrande_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a18aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split 样本数： 160\n",
      "前5个样本：\n",
      "{'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\", 'option1': 'Ian', 'option2': 'Dennis', 'answer': '2'}\n",
      "{'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ enjoyed eating intestine.\", 'option1': 'Ian', 'option2': 'Dennis', 'answer': '1'}\n",
      "{'sentence': 'He never comes to my home, but I always go to his house because the _ is smaller.', 'option1': 'home', 'option2': 'house', 'answer': '1'}\n",
      "{'sentence': 'He never comes to my home, but I always go to his house because the _ is bigger.', 'option1': 'home', 'option2': 'house', 'answer': '2'}\n",
      "{'sentence': \"Kyle doesn't wear leg warmers to bed, while Logan almost always does. _ is more likely to live in a colder climate.\", 'option1': 'Kyle', 'option2': 'Logan', 'answer': '2'}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = winogrande_dataset[\"train\"]\n",
    "print(\"train split 样本数：\", len(train_dataset))\n",
    "print(\"前5个样本：\")\n",
    "for i in range(5):\n",
    "    print(train_dataset[i])\n",
    "    messages = [{'role': 'system', 'content': 'Please reason step by step, and put your final answer within \\\\boxed{{}}.'},\n",
    "                {'role': 'user', 'content': train_dataset[i]['sentence']}]\n",
    "    print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0f2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_sample(item: dict) -> list:\n",
    "    \"\"\"\n",
    "    Processes a data sample to create a question-answering prompt.\n",
    "\n",
    "    The function takes a dictionary representing a WinoGrande-style problem\n",
    "    and formats it into a list of messages suitable for a conversational AI.\n",
    "\n",
    "    Args:\n",
    "        item: A dictionary with keys 'sentence', 'option1', 'option2'.\n",
    "              Example: {'sentence': \"The trophy doesn't fit in the suitcase because _ is too big.\",\n",
    "                        'option1': 'trophy', 'option2': 'suitcase', 'answer': '1'}\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries for the prompt, containing a system and a user message.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert assistant in natural language understanding and commonsense reasoning. Please reason step-by-step and answer with the option number.\n",
    "\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Complete the following sentence by choosing the most logical option to fill in the blank.\n",
    "\n",
    "Sentence: \"{item['sentence']}\"\n",
    "\n",
    "Options:\n",
    "1. {item['option1']}\n",
    "2. {item['option2']}\n",
    "\n",
    "Which option is the correct choice?\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "    \n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d21cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-545b36a5-a1d9-984d-97e8-f79af0f805a1\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Let's reason through this step-by-step:\\n\\n1. The sentence states that Ian volunteered to eat Dennis's menudo, which is an unusual dish containing intestines.\\n2. It also mentions that Ian had already had a bowl of it.\\n3. The sentence ends with \\\"because _ despised eating intestine. \\\"\\n4. We need to determine who despises eating intestines - Ian or Dennis.\\n\\nGiven these points, we can conclude:\\n- Ian has already eaten a bowl of menudo, which contains intestines.\\n- Despite having already eaten some, Ian is still willing to eat more menudo.\\n- This implies that Ian does not despise eating intestines, as he's not refusing to eat them again.\\n\\nTherefore, the blank should be filled with Dennis, as he is the one who would likely despise eating intestine due to his own menudo.\\n\\nThe most logical option is:\\nOption 2: Dennis\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1753625869,\"model\":\"qwen2.5-7b-instruct\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":184,\"prompt_tokens\":95,\"total_tokens\":279,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}\n"
     ]
    }
   ],
   "source": [
    "messages = process_sample(train_dataset[0])\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def get_response():\n",
    "    client = OpenAI(\n",
    "        api_key=dashscope_api_key,  # 如果您没有配置环境变量，请用阿里云百炼API Key将本行替换为：api_key=\"sk-xxx\"\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope SDK的base_url\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen2.5-7b-instruct\",  # 此处以qwen-plus为例，可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        messages=messages,\n",
    "        )\n",
    "    print(completion.model_dump_json())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
